{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0dc3d58",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e60805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.datasets.datasets import get_dataset_by_name\n",
    "from src.modeling.vgg import COB_VGG\n",
    "from src.utils import select_best_device, get_project_root\n",
    "from src.training.train import evaluate\n",
    "from src.modeling.pruning import PruningStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a4557",
   "metadata": {},
   "source": [
    "Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b4092",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_dataset_by_name(\n",
    "    dataset_name=\"cifar-10\",\n",
    "    batch_size=10000,\n",
    "    eval_batch_size=1024,\n",
    "    num_workers=0,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "model_path = get_project_root() / \"pretrained_weights\" / \"cifar_10-tsra_rms\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050e818f",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b362a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_bar_graph(y, figsize=(6,2), dpi=100, xlabel = None, ylabel = None, title = None):\n",
    "    # Create bar graph\n",
    "    plt.figure(figsize=figsize, dpi=dpi)\n",
    "    plt.bar(range(len(y)), y, width=1.0, edgecolor='none')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def graph_importance_scores(model):\n",
    "    for m in model.get_ordered_cob_modules():\n",
    "        importance_scores = m._importance_scores\n",
    "        basic_bar_graph(importance_scores, xlabel=\"Dims\", ylabel=\"Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c66ca42",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee82fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = select_best_device()\n",
    "\n",
    "original_model = COB_VGG.from_pretrained(model_path)\n",
    "original_model.eval()\n",
    "original_model.to(device)\n",
    "\n",
    "data = next(iter(train_loader))[0]\n",
    "data = data.to(device)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bc1ef0",
   "metadata": {},
   "source": [
    "Copy model\n",
    "\n",
    "The copied model is used beyond this point. This way, rapid pruning testing can be done without having to wait to load the model from storage each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b54e31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = copy.deepcopy(original_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615d874f",
   "metadata": {},
   "source": [
    "First eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f7b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, test_loader, device)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0fe2ef",
   "metadata": {},
   "source": [
    "Compute importance + graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde84e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_importance_scores(data)\n",
    "\n",
    "graph_importance_scores(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa34c8a",
   "metadata": {},
   "source": [
    "Rotate + compute importance + graph again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc8da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rotate_for_prune(data)\n",
    "model.compute_importance_scores(data)\n",
    "\n",
    "graph_importance_scores(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c45c7d",
   "metadata": {},
   "source": [
    "Prune + eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: Supplying a tuple of TWO values to PruningStrategy indicates that the respective pruning should take place SEPARATELY within the two SEPARATE subspaces of the activation function. Supplying ONE value indicates the entire hidden space should be considered as one.\n",
    "## Eg. PruningStrategy(proportion=(p,p)) vs PruningStrategy(proportion=p)\n",
    "\n",
    "# ## Prune set % of dimensions from each layer\n",
    "# p_desired_prune = 0.7\n",
    "# p = 1-math.sqrt(1-p_desired_prune)\n",
    "# pruning_strategy = PruningStrategy(proportion=(p,p))\n",
    "\n",
    "## Threshold pruning methods\n",
    "thres = 0.1\n",
    "# pruning_strategy = PruningStrategy(zscore_cutoff=(thres, thres))\n",
    "# pruning_strategy = PruningStrategy(prop_of_avg=(thres, thres))\n",
    "# pruning_strategy = PruningStrategy(prop_of_med=(thres, thres))\n",
    "pruning_strategy = PruningStrategy(prop_of_max=(thres, thres))\n",
    "\n",
    "\n",
    "\n",
    "def parameter_count(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "p_before_pruning = parameter_count(model)\n",
    "\n",
    "for m in model.get_ordered_cob_modules():\n",
    "    results = m.structured_prune(pruning_strategy)\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(f\"Number pruned: {results['pruned_absolute_num']}\")\n",
    "    print(f\"Prop pruned: {results['pruned_prop']}\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "p_after_pruning = parameter_count(model)\n",
    "\n",
    "print(f\"Parameter count before pruning: {p_before_pruning}\")\n",
    "print(f\"Parameter count after pruning: {p_after_pruning}\")\n",
    "print(f\"Parameter count pruned: {p_before_pruning - p_after_pruning}\")\n",
    "print(f\"Percentage pruned: {(p_before_pruning - p_after_pruning) / p_before_pruning}\")\n",
    "\n",
    "evaluate(model, test_loader, device)[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
